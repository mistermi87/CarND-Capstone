{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Performance Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "# from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageColor\n",
    "from PIL import ImageFont\n",
    "import tensorflow as tf\n",
    "import xml.etree.ElementTree as ET\n",
    "from scipy.stats import norm\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"TensorFlow version:\", tf.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model preparation\n",
    "\n",
    "Export a trained model. Set class info. Set path to test images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What model to load\n",
    "# MODEL_NAME = 'training/model1/trained_model'\n",
    "MODEL_NAME = 'training/model2/trained_model'\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join('data/', 'label_map_sdc.pbtxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shoud match with the order in label_map_sdc.pbtxt\n",
    "CLASSNAME_LIST = ['Green', 'Red', 'Yellow'] # list of class name \n",
    "COLOR_LIST = ['lawngreen', 'red', 'yellow'] # list of color to be used for visual purpose below "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Frozen Tensorflow Model into Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "def filter_boxes(min_score, boxes, scores, classes):\n",
    "    \"\"\"Return boxes with a confidence >= `min_score`\"\"\"\n",
    "    n = len(classes)\n",
    "    idxs = []\n",
    "    for i in range(n):\n",
    "        if scores[i] >= min_score:\n",
    "            idxs.append(i)\n",
    "    \n",
    "    filtered_boxes = boxes[idxs, ...]\n",
    "    filtered_scores = scores[idxs, ...]\n",
    "    filtered_classes = classes[idxs, ...]\n",
    "    return filtered_boxes, filtered_scores, filtered_classes\n",
    "\n",
    "def to_image_coords(boxes, height, width):\n",
    "    \"\"\"\n",
    "    The original box coordinate output is normalized, i.e [0, 1].\n",
    "    \n",
    "    This converts it back to the original coordinate based on the image\n",
    "    size.\n",
    "    \"\"\"\n",
    "    box_coords = np.zeros_like(boxes)\n",
    "    box_coords[:, 0] = boxes[:, 0] * height\n",
    "    box_coords[:, 1] = boxes[:, 1] * width\n",
    "    box_coords[:, 2] = boxes[:, 2] * height\n",
    "    box_coords[:, 3] = boxes[:, 3] * width\n",
    "    \n",
    "    return box_coords\n",
    "\n",
    "def draw_boxes(image, boxes, classes, scores, thickness=4):\n",
    "    \"\"\"Draw bounding boxes on the image\"\"\"\n",
    "    image_draw = image.copy()\n",
    "    draw = ImageDraw.Draw(image_draw)\n",
    "    for i in range(len(boxes)):\n",
    "        bot, left, top, right = boxes[i, ...]\n",
    "        class_id = int(classes[i])\n",
    "        color = COLOR_LIST[class_id-1]\n",
    "        cls_name = CLASSNAME_LIST[class_id-1]\n",
    "        percent = str(round(scores[i] * 100, 1))\n",
    "        txt_display = cls_name + \": \" + percent + \"%\"\n",
    "        # print(class_id, cls_name, color, txt_display)\n",
    "        # draw.rectangle([(left, top-15), (left+80, top-thickness)], fill= color)\n",
    "        draw.rectangle([(left-2, bot-15), (left+80, bot)], fill= color)\n",
    "        draw.line([(left, top), (left, bot), (right, bot), (right, top), (left, top)], width=thickness, fill=color)\n",
    "        draw.text((left, bot-15), txt_display, fill=\"black\")\n",
    "    return image_draw\n",
    "\n",
    "def load_graph(graph_file):\n",
    "    \"\"\"Loads a frozen inference graph\"\"\"\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        od_graph_def = tf.GraphDef()\n",
    "        with tf.gfile.GFile(graph_file, 'rb') as fid:\n",
    "            serialized_graph = fid.read()\n",
    "            od_graph_def.ParseFromString(serialized_graph)\n",
    "            tf.import_graph_def(od_graph_def, name='')\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = load_graph(PATH_TO_FROZEN_GRAPH)\n",
    "\n",
    "# The input placeholder for the image.\n",
    "# `get_tensor_by_name` returns the Tensor with the associated name in the Graph.\n",
    "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "# Each box represents a part of the image where a particular object was detected.\n",
    "detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "\n",
    "# Each score represent how level of confidence for each of the objects.\n",
    "# Score is shown on the result image, together with the class label.\n",
    "detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "\n",
    "# The classification of the object (integer id).\n",
    "detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to test image directory \n",
    "TEST_IMAGES_DIR = 'data/v3_sim_data_val'\n",
    "\n",
    "# path to detected images\n",
    "DETECTED_IMAGES_DIR = TEST_IMAGES_DIR + '/detected_images'\n",
    "if not os.path.exists(DETECTED_IMAGES_DIR):\n",
    "        os.makedirs(DETECTED_IMAGES_DIR)\n",
    "\n",
    "# Read source filename_list.txt\n",
    "test_files = open(TEST_IMAGES_DIR + '/filename_list.txt', 'r').read().split('\\n')\n",
    "if '' in test_files:\n",
    "    test_files.remove('')\n",
    "\n",
    "print(\"Using {} test images found in {}/images\".format(len(test_files), TEST_IMAGES_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "minor_errors = 0\n",
    "major_errors = 0\n",
    "\n",
    "for i, filename in enumerate(test_files):\n",
    "\n",
    "    image_path = TEST_IMAGES_DIR + '/images/' + filename + '.jpg'\n",
    "    annotation_path = TEST_IMAGES_DIR + '/annotations/' + filename + '.xml'\n",
    "\n",
    "    # Load the sample image.\n",
    "    image = Image.open(image_path)\n",
    "    image_np = np.expand_dims(np.asarray(image, dtype=np.uint8), 0)\n",
    "    print(\"[{}] Processing {}\".format(i+1, image_path))\n",
    "\n",
    "    ground_truth = {\n",
    "        'Red': 0,\n",
    "        'Green': 0,\n",
    "        'Yellow': 0\n",
    "    }\n",
    "\n",
    "    detection = {\n",
    "        'Red': 0,\n",
    "        'Green': 0,\n",
    "        'Yellow': 0\n",
    "    }\n",
    "\n",
    "    # reading the annotation file\n",
    "    annotation = ET.parse(annotation_path).getroot()\n",
    "    for tl_object in annotation.findall('object'):\n",
    "        tl_name = tl_object.find('name')\n",
    "        ground_truth[tl_name.text] += 1\n",
    "\n",
    "    with tf.Session(graph=detection_graph) as sess:                \n",
    "\n",
    "        # Actual detection.\n",
    "        (boxes, scores, classes) = sess.run([detection_boxes, detection_scores, detection_classes], \n",
    "                                            feed_dict={image_tensor: image_np})\n",
    "        # Remove unnecessary dimensions\n",
    "        boxes = np.squeeze(boxes)\n",
    "        scores = np.squeeze(scores)\n",
    "        classes = np.squeeze(classes)\n",
    "\n",
    "        confidence_cutoff = 0.8\n",
    "        # Filter boxes with a confidence score less than `confidence_cutoff`\n",
    "        boxes, scores, classes = filter_boxes(confidence_cutoff, boxes, scores, classes)\n",
    "\n",
    "        # collecting detection data\n",
    "        for class_id in classes:\n",
    "            detected_class = CLASSNAME_LIST[int(class_id) - 1]\n",
    "            detection[detected_class] += 1\n",
    "\n",
    "        # checking against the ground truth\n",
    "        display_image = False\n",
    "        suffix = None\n",
    "        for light in ground_truth:\n",
    "\n",
    "            if ground_truth[light] != detection[light]:\n",
    "\n",
    "                display_image = True\n",
    "                if ground_truth[light] == 0 or detection[light] == 0:\n",
    "                    major_errors += 1\n",
    "                    suffix = \"major\"\n",
    "                    print(\"   *** Error: there's a significant difference for {} \".format(light) +\n",
    "                          \"(ground truth: {}, detection: {})\".format(ground_truth[light], detection[light]))\n",
    "\n",
    "                else:\n",
    "                    minor_errors += 1\n",
    "                    if suffix != \"major\":\n",
    "                        suffix = \"minor\"\n",
    "                    print(\"   * Warning: there's a difference for {} but it's still OK \".format(light) +\n",
    "                          \"(ground truth: {}, detection: {})\".format(ground_truth[light], detection[light]))\n",
    "\n",
    "        if display_image:\n",
    "            # The current box coordinates are normalized to a range between 0 and 1.\n",
    "            # This converts the coordinates actual location on the image.\n",
    "            width, height = image.size\n",
    "            box_coords = to_image_coords(boxes, height, width)\n",
    "\n",
    "            # Each class with be represented by a differently colored box\n",
    "            image_draw = draw_boxes(image, box_coords, classes, scores)\n",
    "\n",
    "            display(image_draw)\n",
    "\n",
    "            save_image_path = DETECTED_IMAGES_DIR + '/' + filename + \"_detected_\" + suffix + \".jpg\"\n",
    "            image_draw.save(save_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------- data/v2_sim_data_val\n",
    "# # Displaying some statistics\n",
    "# all_count = len(test_files)\n",
    "# passed = all_count - major_errors\n",
    "# print(\"Total image count:\", all_count)\n",
    "# print(\"Minor errors: {} ({:.2f}%)\".format(minor_errors, (100.0 * minor_errors / all_count)))\n",
    "# print(\"Major errors: {} ({:.2f}%)\".format(major_errors, (100.0 * major_errors / all_count)))\n",
    "# print(\"Passed: {} ({:.2f}%))\".format(passed, (100.0 * passed / all_count)))\n",
    "\n",
    "# Total image count: 232\n",
    "# Minor errors: 11 (4.74%)\n",
    "# Major errors: 1 (0.43%)\n",
    "# Passed: 231 (99.57%))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------- data/v3_sim_data_val\n",
    "# Displaying some statistics\n",
    "all_count = len(test_files)\n",
    "passed = all_count - major_errors\n",
    "print(\"Total image count:\", all_count)\n",
    "print(\"Minor errors: {} ({:.2f}%)\".format(minor_errors, (100.0 * minor_errors / all_count)))\n",
    "print(\"Major errors: {} ({:.2f}%)\".format(major_errors, (100.0 * major_errors / all_count)))\n",
    "print(\"Passed: {} ({:.2f}%))\".format(passed, (100.0 * passed / all_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carnd-capstone",
   "language": "python",
   "name": "carnd-capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
